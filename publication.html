<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhihe Lu (Lucas)</title>

  <meta name="author" content="Zhihe Lu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <style>
    p {font-size: 18px;font-family: "Times New Roman", Times, serif;}

    a {font-size: 18px;font-family: "Times New Roman", Times, serif;}

    br {font-size: 18px;font-family: "Times New Roman", Times, serif;}

    em {font-size: 18px;font-family: "Times New Roman", Times, serif;}

    strong {font-size: 18px;font-family: "Times New Roman", Times, serif;}

    papertitle {font-size: 18px;font-family: "Times New Roman", Times, serif;}

    name {font-size: 30px; font-family: "Times New Roman", Times, serif;}

    heading {font-size: 30px; font-family: "Times New Roman", Times, serif;}
  </style>
</head>

<body>

</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/envlm.png" alt="envlm_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://arxiv.org/pdf/2311.17091">
        <papertitle>Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models</papertitle>
      </a>
      <br>
          <strong>Zhihe Lu</strong>,
          <a>Jiawang Bai</a>,
          <a>Xin Li</a>,
          <a>Zeyu Xiao</a>,
          <a>Xinchao Wang</a>
      <br>
      <em>ICML, 2024</em>
      <br>
      <a href="https://github.com/zhiheLu/Ensemble_VLM">code</a>
      <p style="line-height:1.5">The first investigation of ensemble learning for VLMs.</p>
    </td>
  </tr>
  
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/graph.png" alt="graph_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://openreview.net/pdf?id=YmEDnMynuO">
        <papertitle>GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph</papertitle>
      </a>
      <br>
          <a>Xin Li<sup>*</sup></a>,
          <a>Dongze Lian<sup>*</sup></a>,
          <strong>Zhihe Lu<sup>*</sup></strong>,
          <a>Jiawang Bai</a>,
          <a>Zhibo Chen</a>,
          <a>Xinchao Wang</a>
      <br>
      <em>* Equal Contribution</em>
      <br>
      <br>
      <em>NeurIPS, 2023</em>
      <br>
      <a href="https://github.com/lixinustc/GraphAdapter">code</a>
      <p style="line-height:1.5">Introduce knowledge graph for tuning vision and language models.</p>
    </td>
  </tr>
  
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/fda.png" alt="fda_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://openreview.net/pdf?id=eKFrXWb0sT">
        <papertitle>Frequency-enhanced Data Augmentation for Vision-and-Language Navigation</papertitle>
      </a>
      <br>
          <a>Keji He</a>,
          <a>Chenyang Si</a>,
          <strong>Zhihe Lu</strong>,
          <a>Yan Huang</a>,
          <a>Liang Wang</a>,
          <a>Xinchao Wang</a>
      <br>
      <em>NeurIPS, 2023</em>
      <br>
      <a href="https://github.com/hekj/FDA">code</a>
      <p style="line-height:1.5">A work to explore the significance of high-frequency information for enhanced Vision-and-Language Navigation.</p>
    </td>
  </tr>
  
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/sfda.png" alt="pcn_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://ieeexplore.ieee.org/abstract/document/10189399">
        <papertitle>Uncertainty-aware Source-free Domain Adaptive Semantic Segmentation</papertitle>
      </a>
      <br>
          <strong>Zhihe Lu</strong>,
          <a>Da Li</a>,
          <a>Yi-Zhe Song</a>,
          <a>Tao Xiang</a>,
          <a>Timothy M. Hospedales</a>
      <br>
      <em>TIP, 2023</em>
      <br>
      <p style="line-height:1.5">An uncertainty-aware solution for SFDASS.</p>
    </td>
  </tr>
  
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/pcn.png" alt="pcn_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://arxiv.org/pdf/2210.08290.pdf">
        <papertitle>Prediction Calibration for Generalized Few-shot Semantic Segmentation</papertitle>
      </a>
      <br>
          <strong>Zhihe Lu</strong>,
          <a>Sen He</a>,
          <a>Da Li</a>,
          <a>Yi-Zhe Song</a>,
          <a>Tao Xiang</a>
      <br>
      <em>TIP, 2023</em>
      <br>
      <p style="line-height:1.5">Investigating the feature-prediction covariance based Transformer for calibrating the biases in GFSS.</p>
    </td>
  </tr>
  
   <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/TaskRes.png" alt="taskres_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://arxiv.org/pdf/2211.10277.pdf">
        <papertitle>Task Residual for Tuning Vision-Language Models</papertitle>
      </a>
      <br>
          <a>Tao Yu<sup>*</sup></a>,
          <strong>Zhihe Lu<sup>*</sup></strong>,
          <a>Xin Jin</a>,
          <a>Zhibo Chen</a>,
          <a>Xinchao Wang</a>
      <br>
      <em>* Equal Contribution</em>
      <br>
      <br>
      <em>CVPR, 2023</em>
      <br>
      <a href="https://github.com/geekyutao/TaskRes">code</a>
      <p style="line-height:1.5">A simple yet effective tuning method for vision and language models.</p>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/cwt_frame.jpg" alt="cwt_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://arxiv.org/pdf/2108.03032.pdf">
        <papertitle>Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer</papertitle>
      </a>
      <br>
          <strong>Zhihe Lu</strong>,
          <a>Sen He</a>,
          <a>Xiatian Zhu</a>,
          <a>Li Zhang</a>,
          <a>Yi-Zhe Song</a>,
          <a>Tao Xiang</a>
      <br>
      <em>ICCV, 2021</em>
      <br>
      <a href="https://github.com/zhiheLu/CWT-for-FSS">code</a>
      <p style="line-height:1.5">A novel training pipeline for few-shot segmentation with classifier weight transformer.</p>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/star_frame.jpg" alt="star_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Stochastic_Classifiers_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf">
        <papertitle>Stochastic Classifiers for Unsupervised Domain Adaptation</papertitle>
      </a>
      <br>
          <strong>Zhihe Lu</strong>,
          <a>Yongxin Yang</a>,
          <a>Xiatian Zhu</a>,
          <a>Cong Liu</a>,
          <a>Yi-Zhe Song</a>,
          <a>Tao Xiang</a>
      <br>
      <em>CVPR, 2020</em>
      <br>
      <a href="https://github.com/zhiheLu/STAR_Stochastic_Classifiers_for_UDA">code</a>
      <p style="line-height:1.5">A novel way to use infinite number of classifiers without extra parameters to identify misaligned regions.</p>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/gafp_gan.jpg" alt="gafp_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://dl.acm.org/doi/pdf/10.1145/3240508.3240647">
        <papertitle>Conditional Expression Synthesis with Face Parsing Transformation</papertitle>
      </a>
      <br>
          <strong>Zhihe Lu</strong>,
          <a>Tanhao Hu</a>,
          <a>Lingxiao Song</a>,
          <a>Zhaoxiang Zhang</a>,
          <a>Ran He</a>
      <br>
      <em>ACM MM, 2018</em>
      <br>
      <p style="line-height:1.5">A Couple-Agent Face Parsing based Generative Adversarial Network (CAFP-GAN) that unites the knowledge of facial semantic regions and controllable expression signals.</p>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/g2_gan.jpg" alt="g2_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://arxiv.org/abs/1712.03474">
        <papertitle>Geometry Guided Adversarial Facial Expression Synthesis</papertitle>
      </a>
      <br>
          <a>Lingxiao Song</a>,
          <strong>Zhihe Lu</strong>,
          <a>Ran He</a>,
          <a>Zhenan Sun</a>,
          <a>Tieniu Tan</a>
      <br>
      <em>ACM MM, 2018</em>
      <br>
      <p style="line-height:1.5"> A Geometry-Guided Generative Adversarial Network (G2-GAN) was proposed for photorealistic and identity-preserving facial expression synthesis.</p>
    </td>
  </tr>

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/face_survey.jpg" alt="face_survey_png" width="250" style="border-style: none">
    </td>
    <td width="70%" valign="middle">
      <a href="https://arxiv.org/abs/1706.04717">
        <papertitle>Recent Progress of Face Image Synthesis</papertitle>
      </a>
      <br>
      <strong>Zhihe Lu</strong>,
      <a>Zhihang Li</a>,
      <a>Jie Cao</a>,
      <a>Ran He</a>,
      <a>Zhenan Sun</a>
      <br>
      <em>ACPR, 2017</em>
      <br>
      <p style="line-height:1.5"> A very early survey for face image synthesis.</p>
    </td>
  </tr>

</tbody></table>

</body>

</html>